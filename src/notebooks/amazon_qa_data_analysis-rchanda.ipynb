{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run ../utils/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Video_Games\"\n",
    "C.INPUT_DATA_PATH = '../../data/90_input'\n",
    "with open('%s/train-%s.pickle' % (C.INPUT_DATA_PATH, category), 'rb') as f:\n",
    "    train_data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    punctuations = string.punctuation.replace(\"\\'\", '')\n",
    "    \n",
    "    for ch in punctuations:\n",
    "        text = text.replace(ch, \" \"+ch+\" \")\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        if token.isupper() == False:\n",
    "            tokens[i] = token.lower()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'hello', '!', 'CMU', '.', \"don't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Hi, Hello! CMU. don't \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    questionsList = row['questionsList']\n",
    "    for question in questionsList:\n",
    "        if 'text' in question:\n",
    "            text = question['text']\n",
    "            vocab.add_sequence(tokenize(text))\n",
    "\n",
    "            for answer in question['answers']:\n",
    "                text = answer['text']\n",
    "                vocab.add_sequence(tokenize(text))\n",
    "    \n",
    "    reviewsList = row['reviewsList']\n",
    "    for review in reviewsList:\n",
    "        text = review['text']\n",
    "        vocab.add_sequence(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab.sort_vocabulary()\n",
    "vocab._token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reviews row to json\n",
    "def reviewsToIds(row):\n",
    "    reviewIdsList = []\n",
    "    for review in row:\n",
    "        reviewJson = {}\n",
    "        text = C.SOS + review[C.TEXT] + C.EOS\n",
    "        reviewJson[C.IDS] = vocab.indices_from_sequence(tokenize(text))\n",
    "        reviewIdsList.append(reviewJson)\n",
    "\n",
    "    return reviewIdsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questionsToIds(questions_list):\n",
    "    new_questions_list = []\n",
    "    for question in questions_list:\n",
    "        if C.TEXT in question:\n",
    "            new_question = {}\n",
    "            text = C.SOS + question[C.TEXT] + C.EOS\n",
    "            new_question[C.IDS] = vocab.indices_from_sequence(tokenize(text))\n",
    "\n",
    "            new_answers = []\n",
    "            for answer in question[C.ANSWERS]:\n",
    "                new_answer = {}\n",
    "                text = C.SOS + answer[C.TEXT] + C.EOS\n",
    "                new_answer[C.IDS] = vocab.indices_from_sequence(tokenize(text))\n",
    "            new_answers.append(new_answer)\n",
    "\n",
    "            new_question[C.ANSWER_IDS_LIST] = new_answers\n",
    "            new_questions_list.append(new_question)\n",
    "\n",
    "    return new_questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[C.REVIEW_IDS_LIST] = train_data[C.REVIEWS_LIST].apply(reviewsToIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[C.QUESTION_IDS_LIST] = train_data[C.QUESTIONS_LIST].apply(questionsToIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answerIdsList': [{'ids': [1, 2758, 27, 6, 4089, 4, 1]}],\n",
       "  'ids': [1, 14, 14, 16, 327, 22, 319, 302, 63, 1]},\n",
       " {'answerIdsList': [{'ids': [1,\n",
       "     86,\n",
       "     322,\n",
       "     7,\n",
       "     75,\n",
       "     55,\n",
       "     318,\n",
       "     20,\n",
       "     355,\n",
       "     1731,\n",
       "     42,\n",
       "     41,\n",
       "     4,\n",
       "     1]}],\n",
       "  'ids': [1, 14, 16, 114, 22, 319, 322, 63, 1]},\n",
       " {'answerIdsList': [{'ids': [1, 660, 1]}],\n",
       "  'ids': [1, 67, 10, 23, 8, 86, 505, 63, 1]},\n",
       " {'answerIdsList': [{'ids': [1,\n",
       "     15,\n",
       "     266,\n",
       "     9,\n",
       "     586,\n",
       "     19,\n",
       "     7,\n",
       "     11,\n",
       "     49,\n",
       "     560,\n",
       "     22,\n",
       "     505,\n",
       "     7,\n",
       "     15,\n",
       "     40,\n",
       "     134,\n",
       "     8,\n",
       "     207,\n",
       "     5,\n",
       "     420,\n",
       "     89,\n",
       "     12,\n",
       "     19,\n",
       "     5,\n",
       "     461,\n",
       "     5,\n",
       "     58,\n",
       "     941,\n",
       "     13,\n",
       "     5,\n",
       "     1]}],\n",
       "  'ids': [1,\n",
       "   11,\n",
       "   9,\n",
       "   1108,\n",
       "   15,\n",
       "   266,\n",
       "   19,\n",
       "   44,\n",
       "   369,\n",
       "   6,\n",
       "   7,\n",
       "   32,\n",
       "   31,\n",
       "   6,\n",
       "   110,\n",
       "   11,\n",
       "   560,\n",
       "   20,\n",
       "   505,\n",
       "   63,\n",
       "   42,\n",
       "   55,\n",
       "   4,\n",
       "   1]},\n",
       " {'answerIdsList': [{'ids': [1, 28, 10, 23, 181, 361, 11, 22, 1, 4, 1]}],\n",
       "  'ids': [1, 14, 16, 327, 22, 319, 322, 63, 1]},\n",
       " {'answerIdsList': [{'ids': [1,\n",
       "     76,\n",
       "     783,\n",
       "     1093,\n",
       "     5,\n",
       "     16,\n",
       "     11,\n",
       "     991,\n",
       "     9,\n",
       "     258,\n",
       "     420,\n",
       "     191,\n",
       "     36,\n",
       "     76,\n",
       "     207,\n",
       "     1]}],\n",
       "  'ids': [1, 15, 23, 8, 344, 5, 16, 13, 5, 1108, 4, 1]},\n",
       " {'answerIdsList': [{'ids': [1,\n",
       "     15,\n",
       "     134,\n",
       "     505,\n",
       "     69,\n",
       "     79,\n",
       "     7,\n",
       "     32,\n",
       "     15,\n",
       "     459,\n",
       "     23,\n",
       "     389,\n",
       "     1523,\n",
       "     35,\n",
       "     9943,\n",
       "     11,\n",
       "     49,\n",
       "     34,\n",
       "     9,\n",
       "     1323,\n",
       "     19,\n",
       "     5,\n",
       "     3293,\n",
       "     8,\n",
       "     289,\n",
       "     8,\n",
       "     344,\n",
       "     11,\n",
       "     328,\n",
       "     5,\n",
       "     16,\n",
       "     6,\n",
       "     21,\n",
       "     11,\n",
       "     49,\n",
       "     34,\n",
       "     201,\n",
       "     11,\n",
       "     6,\n",
       "     14,\n",
       "     16,\n",
       "     12,\n",
       "     5,\n",
       "     113,\n",
       "     29,\n",
       "     1]}],\n",
       "  'ids': [1,\n",
       "   25,\n",
       "   5,\n",
       "   1,\n",
       "   17,\n",
       "   14,\n",
       "   16,\n",
       "   7,\n",
       "   110,\n",
       "   11,\n",
       "   891,\n",
       "   4548,\n",
       "   8,\n",
       "   41,\n",
       "   474,\n",
       "   320,\n",
       "   226,\n",
       "   63,\n",
       "   1]},\n",
       " {'answerIdsList': [{'ids': [1,\n",
       "     27,\n",
       "     197,\n",
       "     21,\n",
       "     543,\n",
       "     179,\n",
       "     289,\n",
       "     11,\n",
       "     6,\n",
       "     76,\n",
       "     23,\n",
       "     9,\n",
       "     319,\n",
       "     302,\n",
       "     7,\n",
       "     11,\n",
       "     145,\n",
       "     603,\n",
       "     31,\n",
       "     232,\n",
       "     11,\n",
       "     9,\n",
       "     289,\n",
       "     69,\n",
       "     39,\n",
       "     1]}],\n",
       "  'ids': [1, 14, 16, 114, 22, 319, 1]}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[C.QUESTION_IDS_LIST].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "mode = \"3\"\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    questionIdsList = row[C.QUESTION_IDS_LIST]\n",
    "    for question in questionIdsList:\n",
    "        tup = ()\n",
    "        \n",
    "        if mode is not \"1\":\n",
    "            ids = question[C.IDS]\n",
    "            tup += (ids,)\n",
    "\n",
    "        for answer in question[C.ANSWER_IDS_LIST]:\n",
    "            ids = answer[C.IDS]\n",
    "            data.append(tup+(ids,))\n",
    "    \n",
    "    if mode is \"3\":\n",
    "        reviewsList = row[C.REVIEW_IDS_LIST]\n",
    "        reviewIds = []\n",
    "        \n",
    "        for review in reviewsList[0:2]:\n",
    "            ids = review[C.IDS]\n",
    "            reviewIds.append(ids)\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            data[i] += (reviewIds,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  12,\n",
       "  9,\n",
       "  47,\n",
       "  16,\n",
       "  4,\n",
       "  21,\n",
       "  28,\n",
       "  135,\n",
       "  81,\n",
       "  2177,\n",
       "  6,\n",
       "  11,\n",
       "  49,\n",
       "  891,\n",
       "  9,\n",
       "  130,\n",
       "  13,\n",
       "  5699,\n",
       "  199,\n",
       "  11,\n",
       "  1535,\n",
       "  2460,\n",
       "  4,\n",
       "  11,\n",
       "  12,\n",
       "  82,\n",
       "  201,\n",
       "  1251,\n",
       "  17,\n",
       "  6,\n",
       "  31,\n",
       "  170,\n",
       "  28,\n",
       "  215,\n",
       "  1884,\n",
       "  8,\n",
       "  353,\n",
       "  17,\n",
       "  1758,\n",
       "  4,\n",
       "  5,\n",
       "  59,\n",
       "  2390,\n",
       "  259,\n",
       "  12,\n",
       "  18,\n",
       "  5,\n",
       "  16,\n",
       "  2481,\n",
       "  53,\n",
       "  1710,\n",
       "  14,\n",
       "  7,\n",
       "  12,\n",
       "  2594,\n",
       "  1758,\n",
       "  611,\n",
       "  4,\n",
       "  28,\n",
       "  182,\n",
       "  1181,\n",
       "  662,\n",
       "  16,\n",
       "  199,\n",
       "  11,\n",
       "  6,\n",
       "  1,\n",
       "  12,\n",
       "  8777,\n",
       "  72,\n",
       "  844,\n",
       "  4,\n",
       "  14,\n",
       "  65,\n",
       "  6,\n",
       "  5,\n",
       "  844,\n",
       "  25,\n",
       "  263,\n",
       "  19,\n",
       "  5,\n",
       "  1,\n",
       "  5550,\n",
       "  6,\n",
       "  19,\n",
       "  4247,\n",
       "  6,\n",
       "  6807,\n",
       "  7,\n",
       "  868,\n",
       "  13,\n",
       "  2391,\n",
       "  4,\n",
       "  844,\n",
       "  41,\n",
       "  56,\n",
       "  386,\n",
       "  6340,\n",
       "  18,\n",
       "  5,\n",
       "  168,\n",
       "  301,\n",
       "  4,\n",
       "  6340,\n",
       "  25,\n",
       "  2186,\n",
       "  72,\n",
       "  5,\n",
       "  73,\n",
       "  320,\n",
       "  521,\n",
       "  73,\n",
       "  386,\n",
       "  844,\n",
       "  6,\n",
       "  191,\n",
       "  9,\n",
       "  3966,\n",
       "  16,\n",
       "  569,\n",
       "  56,\n",
       "  4,\n",
       "  140,\n",
       "  5,\n",
       "  3966,\n",
       "  1918,\n",
       "  110,\n",
       "  27,\n",
       "  912,\n",
       "  1950,\n",
       "  923,\n",
       "  33,\n",
       "  35,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  205,\n",
       "  6,\n",
       "  1,\n",
       "  12,\n",
       "  2803,\n",
       "  78,\n",
       "  5,\n",
       "  1647,\n",
       "  8378,\n",
       "  33,\n",
       "  4,\n",
       "  21,\n",
       "  5,\n",
       "  4698,\n",
       "  30,\n",
       "  7,\n",
       "  3062,\n",
       "  5,\n",
       "  230,\n",
       "  6,\n",
       "  297,\n",
       "  65,\n",
       "  844,\n",
       "  4,\n",
       "  5,\n",
       "  2481,\n",
       "  53,\n",
       "  100,\n",
       "  598,\n",
       "  2118,\n",
       "  481,\n",
       "  8,\n",
       "  5,\n",
       "  8378,\n",
       "  205,\n",
       "  6,\n",
       "  89,\n",
       "  12,\n",
       "  712,\n",
       "  579,\n",
       "  6,\n",
       "  68,\n",
       "  7,\n",
       "  2699,\n",
       "  4,\n",
       "  169,\n",
       "  100,\n",
       "  6,\n",
       "  10,\n",
       "  131,\n",
       "  401,\n",
       "  8,\n",
       "  126,\n",
       "  5,\n",
       "  16,\n",
       "  114,\n",
       "  225,\n",
       "  6,\n",
       "  111,\n",
       "  8379,\n",
       "  22,\n",
       "  505,\n",
       "  8,\n",
       "  2490,\n",
       "  7,\n",
       "  454,\n",
       "  11,\n",
       "  4,\n",
       "  14,\n",
       "  53,\n",
       "  27,\n",
       "  1009,\n",
       "  82,\n",
       "  6,\n",
       "  7,\n",
       "  10,\n",
       "  131,\n",
       "  746,\n",
       "  8,\n",
       "  142,\n",
       "  18,\n",
       "  5,\n",
       "  16,\n",
       "  12,\n",
       "  1038,\n",
       "  2104,\n",
       "  4,\n",
       "  21,\n",
       "  10,\n",
       "  131,\n",
       "  197,\n",
       "  18,\n",
       "  64,\n",
       "  11,\n",
       "  12,\n",
       "  138,\n",
       "  57,\n",
       "  7,\n",
       "  498,\n",
       "  6,\n",
       "  2461,\n",
       "  176,\n",
       "  13,\n",
       "  68,\n",
       "  3475,\n",
       "  8,\n",
       "  34,\n",
       "  75,\n",
       "  4,\n",
       "  1],\n",
       " [1,\n",
       "  151,\n",
       "  14,\n",
       "  22,\n",
       "  505,\n",
       "  20,\n",
       "  38,\n",
       "  555,\n",
       "  685,\n",
       "  464,\n",
       "  17,\n",
       "  155,\n",
       "  3034,\n",
       "  4,\n",
       "  913,\n",
       "  4,\n",
       "  14,\n",
       "  16,\n",
       "  12,\n",
       "  231,\n",
       "  29,\n",
       "  712,\n",
       "  230,\n",
       "  4,\n",
       "  238,\n",
       "  72,\n",
       "  14,\n",
       "  16,\n",
       "  12,\n",
       "  47,\n",
       "  4,\n",
       "  1]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    A Vocabulary stores a set of words in the corpus mapped to unique integer IDs.\n",
    "\n",
    "    In addition to the words in the actual language, a Vocabulary includes three\n",
    "    reserved tokens (and IDs) for the start-of-sequence and end-of-sequence\n",
    "    markers, and for a special 'UNK' marker used to handle rare/unknown words.\n",
    "\n",
    "    The Vocabulary is sorted in descending order based on frequency. If the\n",
    "    number of words seen is greater than the maximum size of the Vocabulary,\n",
    "    the remaining least-frequent words are ignored.\n",
    "\n",
    "    Args: size(int): maximum number of words allowed in this vocabulary\n",
    "    \"\"\"\n",
    "    def __init__(self, max_vocab_size):\n",
    "        self.PAD_token_name = \"<PAD>\"\n",
    "        self.UNK_token_name = \"<UNK>\"\n",
    "        self.SOS_token_name = \"<SOS>\"\n",
    "        self.EOS_token_name = \"<EOS>\"\n",
    "        self.PAD_token_id = 0\n",
    "        self.UNK_token_id = 1\n",
    "        self.SOS_token_id = 2\n",
    "        self.EOS_token_id = 3\n",
    "\n",
    "        self._reserved = set([self.PAD_token_name, self.UNK_token_name, \\\n",
    "                self.SOS_token_name, self.EOS_token_name])\n",
    "        self._reserved_token_id = [\n",
    "                (self.PAD_token_name, self.PAD_token_id),\n",
    "                (self.UNK_token_name, self.UNK_token_id),\n",
    "                (self.SOS_token_name, self.SOS_token_id),\n",
    "                (self.EOS_token_name, self.EOS_token_id)\n",
    "        ]\n",
    "\n",
    "        self._token2index = dict([(tok, idx) for tok, idx in self._reserved_token_id])\n",
    "        self._index2token = dict([(idx, tok) for tok, idx in self._reserved_token_id])\n",
    "\n",
    "        self._token2count = {}\n",
    "\n",
    "        self._num_tokens = 0\n",
    "        self._num_reserved = 4\n",
    "        self.sorted = False\n",
    "        self.size = max_vocab_size\n",
    "\n",
    "    def trim(self):\n",
    "        \"\"\"\n",
    "        Sorts the vocabulary in descending order based on frequency\n",
    "        \"\"\"\n",
    "        sorted_vocab_count = sorted(self._token2count.items(), \\\n",
    "                key=lambda x: x[1], reverse=True)[:self.size]\n",
    "        self._token2index = dict( [ (w, self._num_reserved + idx) \\\n",
    "                for idx, (w, _) in enumerate(sorted_vocab_count) ] )\n",
    "        self._index2token = dict( [ (idx, w) \\\n",
    "                for w, idx in self._token2index.items() ])\n",
    "\n",
    "        for tok, idx in self._reserved_token_id:\n",
    "            self._token2index[tok] = idx\n",
    "            self._index2token[idx] = tok\n",
    "\n",
    "        if self._num_tokens > self.size:\n",
    "            self._num_tokens = self.size\n",
    "\n",
    "        self.sorted = True\n",
    "\n",
    "    def sort_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Sorts the vocabulary (if it is not already sorted).\n",
    "        \"\"\"\n",
    "        if not self.sorted:\n",
    "            self.trim()\n",
    "\n",
    "\n",
    "    def get_index(self, token):\n",
    "        \"\"\"\n",
    "        Returns: int: ID of the given token.\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return self._token2index[token]\n",
    "\n",
    "\n",
    "    def get_token(self, index):\n",
    "        \"\"\"\n",
    "        Returns: str: token with ID equal to the given index.\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return self._index2token[index]\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        Returns: int: maximum number of words in the vocabulary.\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return self._num_tokens + self._num_reserved\n",
    "\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Adds an occurrence of a token to the vocabulary,\n",
    "        incrementing its observed frequency if the word already exists.\n",
    "        Args: token (int): word to add\n",
    "        \"\"\"\n",
    "        if token in self._reserved:\n",
    "            return\n",
    "        if token not in self._token2count:\n",
    "            self._token2count[token] = 1\n",
    "            self._num_tokens += 1\n",
    "        else:\n",
    "            self._token2count[token] += 1\n",
    "        self.sorted = False\n",
    "\n",
    "    def add_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Adds a sequence of words to the vocabulary.\n",
    "        Args: sequence(list(str)): list of words, e.g. representing a sentence.\n",
    "        \"\"\"\n",
    "        for tok in sequence:\n",
    "            self.add_token(tok)\n",
    "\n",
    "    def indices_from_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Maps a list of words to their token IDs, or else <UNK>\n",
    "        if the word is rare/unknown.\n",
    "        Args: sequence (list(str)): list of words to map\n",
    "        Returns: list(int): list of mapped IDs\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return [self._token2index[tok]\n",
    "                if tok in self._token2index\n",
    "                else self.UNK_token_id\n",
    "                for tok in sequence]\n",
    "\n",
    "    def sequence_from_indices(self, indices):\n",
    "        \"\"\"\n",
    "        Recover a sentence from a list of token IDs.\n",
    "        Args: indices (list(int)): list of token IDs.\n",
    "        Returns: list(str): recovered sentence, represented as a list of words\n",
    "        \"\"\"\n",
    "        seq = [self._index2token[idx] for idx in indices]\n",
    "        return seq\n",
    "\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, self.__class__):\n",
    "            return False\n",
    "        self.sort_vocabulary()\n",
    "        other.sort_vocabulary()\n",
    "\n",
    "        if self._token2count == other._token2count \\\n",
    "                and self._token2index == other._token2index \\\n",
    "                and self._index2token == other._index2token:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self._token2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import numpy as np\n",
    "from operator import itemgetter, attrgetter\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews1 = [[1,2], [1,2,3],[6,7,8]]\n",
    "reviews2 = [[1], [1,2]]\n",
    "\n",
    "question1 = [1,2,3]\n",
    "question2 = [2,3,4,5]\n",
    "\n",
    "answer1 = [1,3]\n",
    "answer2 = [2]\n",
    "\n",
    "data1 = [(answer1), (answer2)]\n",
    "data2 = [(answer1, question1), (answer2, question2)]\n",
    "data3 = [(answer1, question1, reviews1), (answer2, question2, reviews2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, q = zip(*data2)\n",
    "print(a)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataLoader(DataLoader):\n",
    "\n",
    "    def sortByLength(self, item):\n",
    "        if self.mode is \"1\":\n",
    "            return len(item)\n",
    "        \n",
    "        elif self.mode is \"2\":\n",
    "            assert(len(item) == 2)\n",
    "            return len(item[0])\n",
    "        \n",
    "        elif self.mode is \"3\":\n",
    "            assert(len(item) == 3)\n",
    "            reviews = item[2]\n",
    "            max_len = 0\n",
    "            for review in reviews:\n",
    "                max_len = max(max_len, len(review))\n",
    "            return max_len\n",
    "\n",
    "        \n",
    "    def __init__(self, data, mode, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        sorted(data, key=self.sortByLength, reverse=True)    \n",
    "        self.data = data\n",
    "        \n",
    "\n",
    "    def create_packed_qa(self, batch_data):\n",
    "        lengths = np.array([len(item) for item in batch_data])\n",
    "        max_len = max(lengths)\n",
    "        \n",
    "        padded_data = np.array( [np.pad(item, (0, max_len-len(item)), 'constant') for item in batch_data] )\n",
    "        padded_data = torch.from_numpy(padded_data)\n",
    "        \n",
    "        return (padded_data)\n",
    "    \n",
    "    def create_packed_reviews(self, review_data):\n",
    "        max_num_reviews = 0\n",
    "        for reviews in review_data:\n",
    "            max_num_reviews = max(max_num_reviews, len(reviews))\n",
    "        \n",
    "        data = []\n",
    "        for i in range(max_num_reviews):\n",
    "            batch_data = []\n",
    "            for j in range(self.batch_size):\n",
    "                reviews = review_data[j]\n",
    "                if i < len(reviews):\n",
    "                    batch_data.append(reviews[i])\n",
    "                else:\n",
    "                    batch_data.append([0])\n",
    "            data.append(batch_data)\n",
    "        \n",
    "        padded_data = []\n",
    "        for i in range(max_num_reviews):\n",
    "            batch_data = data[i]\n",
    "            lengths = [len(review) for review in batch_data]\n",
    "            max_len = max(lengths)\n",
    "            \n",
    "            padded_batch_data = np.array([np.pad(item, (0, max_len-len(item)), 'constant') for item in batch_data])\n",
    "            padded_batch_data = torch.from_numpy(padded_batch_data)\n",
    "            \n",
    "            padded_data.append(padded_batch_data)\n",
    "            \n",
    "        return padded_data\n",
    "        \n",
    "    def __iter__(self):\n",
    "        print(self.data)\n",
    "        self.num_batches = len(self.data) // self.batch_size\n",
    "        indices = np.arange(self.num_batches)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for index in indices:\n",
    "            start = index*self.batch_size\n",
    "            end = (index+1)*self.batch_size\n",
    "            \n",
    "            batch_data = self.data[start:end]\n",
    "            assert(self.batch_size == len(batch_data))\n",
    "            \n",
    "            if self.mode is \"1\":\n",
    "                answers = batch_data\n",
    "                packed_answers = self.create_packed_qa(answers)\n",
    "                yield (packed_answers)\n",
    "                \n",
    "            elif self.mode is \"2\":\n",
    "                answers, questions = zip(*batch_data)\n",
    "                packed_answers = self.create_packed_qa(list(answers))\n",
    "                packed_questions = self.create_packed_qa(list(questions))\n",
    "                yield (packed_answers, packed_questions)\n",
    "            \n",
    "            elif self.mode is \"3\":\n",
    "                answers, questions, reviews = zip(*batch_data)\n",
    "                packed_answers = self.create_packed_qa(list(answers))\n",
    "                packed_questions = self.create_packed_qa(list(questions))\n",
    "                packed_reviews = self.create_packed_reviews(list(reviews))\n",
    "                yield (packed_answers, packed_questions, packed_reviews)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = AmazonDataLoader(data3, \"3\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(data_loader):\n",
    "    answers, questions, reviews = data\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortByMaxReviewLength(item):\n",
    "    reviews = item[2]\n",
    "    max_len = 0\n",
    "    for review in reviews:\n",
    "        max_len = max(max_len, review.shape[0])\n",
    "    return max_len\n",
    "\n",
    "for item in data3:\n",
    "    assert(len(item) == 3)\n",
    "\n",
    "sorted(data3, key=sortByMaxReviewLength, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortByAnswerLength(item):\n",
    "    return item[0].shape[0]\n",
    "\n",
    "sorted(data2, key=sortByAnswerLength, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = data1\n",
    "print(answers)\n",
    "\n",
    "answers, questions = zip(*data2)\n",
    "print(data2)\n",
    "print(answers)\n",
    "print(questions)\n",
    "\n",
    "answers, questions, reviews = zip(*data3)\n",
    "print(data3)\n",
    "print(answers)\n",
    "print(questions)\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(10, 10, 30))\n",
    "lens = list(range(1, 11))\n",
    "x = pack_padded_sequence(x, lens[::-1], batch_first=True)\n",
    "y = pad_packed_sequence(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
