{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run ../utils/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Video_Games\"\n",
    "C.INPUT_DATA_PATH = '../../data/90_input'\n",
    "with open('%s/train-%s.pickle' % (C.INPUT_DATA_PATH, category), 'rb') as f:\n",
    "    train_data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    punctuations = string.punctuation.replace(\"\\'\", '')\n",
    "    \n",
    "    for ch in punctuations:\n",
    "        text = text.replace(ch, \" \"+ch+\" \")\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        if token.isupper() == False:\n",
    "            tokens[i] = token.lower()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"Hi, Hello! CMU. don't \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    questionsList = row['questionsList']\n",
    "    for question in questionsList:\n",
    "        if 'text' in question:\n",
    "            text = question['text']\n",
    "            vocab.add_sequence(tokenize(text))\n",
    "\n",
    "            for answer in question['answers']:\n",
    "                text = answer['text']\n",
    "                vocab.add_sequence(tokenize(text))\n",
    "    \n",
    "    reviewsList = row['reviewsList']\n",
    "    for review in reviewsList:\n",
    "        text = review['text']\n",
    "        vocab.add_sequence(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab.sort_vocabulary()\n",
    "vocab._token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reviews row to json\n",
    "def reviewsToIds(row):\n",
    "    reviewIdsList = []\n",
    "    for review in row:\n",
    "        reviewJson = {}\n",
    "        text = C.SOS + review[C.TEXT] + C.EOS\n",
    "        reviewJson[C.IDS] = vocab.indices_from_sequence(tokenize(text))\n",
    "        reviewIdsList.append(reviewJson)\n",
    "\n",
    "    return reviewIdsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questionsToIds(questions_list):\n",
    "    new_questions_list = []\n",
    "    for question in questions_list:\n",
    "        if C.TEXT in question:\n",
    "            new_question = {}\n",
    "            text = C.SOS + question[C.TEXT] + C.EOS\n",
    "            new_question[C.IDS] = vocab.indices_from_sequence(tokenize(text))\n",
    "\n",
    "            new_answers = []\n",
    "            for answer in question[C.ANSWERS]:\n",
    "                new_answer = {}\n",
    "                text = C.SOS + answer[C.TEXT] + C.EOS\n",
    "                new_answer[C.IDS] = vocab.indices_from_sequence(tokenize(text))\n",
    "            new_answers.append(new_answer)\n",
    "\n",
    "            new_question[C.ANSWER_IDS_LIST] = new_answers\n",
    "            new_questions_list.append(new_question)\n",
    "\n",
    "    return new_questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[C.REVIEW_IDS_LIST] = train_data[C.REVIEWS_LIST].apply(reviewsToIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[C.QUESTION_IDS_LIST] = train_data[C.QUESTIONS_LIST].apply(questionsToIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[C.QUESTION_IDS_LIST].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "mode = \"3\"\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    questionIdsList = row[C.QUESTION_IDS_LIST]\n",
    "    for question in questionIdsList:\n",
    "        tup = ()\n",
    "        \n",
    "        if mode is not \"1\":\n",
    "            ids = question[C.IDS]\n",
    "            tup += (ids,)\n",
    "\n",
    "        for answer in question[C.ANSWER_IDS_LIST]:\n",
    "            ids = answer[C.IDS]\n",
    "            data.append(tup+(ids,))\n",
    "    \n",
    "    if mode is \"3\":\n",
    "        reviewsList = row[C.REVIEW_IDS_LIST]\n",
    "        reviewIds = []\n",
    "        \n",
    "        for review in reviewsList[0:2]:\n",
    "            ids = review[C.IDS]\n",
    "            reviewIds.append(ids)\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            data[i] += (reviewIds,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    A Vocabulary stores a set of words in the corpus mapped to unique integer IDs.\n",
    "\n",
    "    In addition to the words in the actual language, a Vocabulary includes three\n",
    "    reserved tokens (and IDs) for the start-of-sequence and end-of-sequence\n",
    "    markers, and for a special 'UNK' marker used to handle rare/unknown words.\n",
    "\n",
    "    The Vocabulary is sorted in descending order based on frequency. If the\n",
    "    number of words seen is greater than the maximum size of the Vocabulary,\n",
    "    the remaining least-frequent words are ignored.\n",
    "\n",
    "    Args: size(int): maximum number of words allowed in this vocabulary\n",
    "    \"\"\"\n",
    "    def __init__(self, max_vocab_size):\n",
    "        self.PAD_token_name = \"<PAD>\"\n",
    "        self.UNK_token_name = \"<UNK>\"\n",
    "        self.SOS_token_name = \"<SOS>\"\n",
    "        self.EOS_token_name = \"<EOS>\"\n",
    "        self.PAD_token_id = 0\n",
    "        self.UNK_token_id = 1\n",
    "        self.SOS_token_id = 2\n",
    "        self.EOS_token_id = 3\n",
    "\n",
    "        self._reserved = set([self.PAD_token_name, self.UNK_token_name, \\\n",
    "                self.SOS_token_name, self.EOS_token_name])\n",
    "        self._reserved_token_id = [\n",
    "                (self.PAD_token_name, self.PAD_token_id),\n",
    "                (self.UNK_token_name, self.UNK_token_id),\n",
    "                (self.SOS_token_name, self.SOS_token_id),\n",
    "                (self.EOS_token_name, self.EOS_token_id)\n",
    "        ]\n",
    "\n",
    "        self._token2index = dict([(tok, idx) for tok, idx in self._reserved_token_id])\n",
    "        self._index2token = dict([(idx, tok) for tok, idx in self._reserved_token_id])\n",
    "\n",
    "        self._token2count = {}\n",
    "\n",
    "        self._num_tokens = 0\n",
    "        self._num_reserved = 4\n",
    "        self.sorted = False\n",
    "        self.size = max_vocab_size\n",
    "\n",
    "    def trim(self):\n",
    "        \"\"\"\n",
    "        Sorts the vocabulary in descending order based on frequency\n",
    "        \"\"\"\n",
    "        sorted_vocab_count = sorted(self._token2count.items(), \\\n",
    "                key=lambda x: x[1], reverse=True)[:self.size]\n",
    "        self._token2index = dict( [ (w, self._num_reserved + idx) \\\n",
    "                for idx, (w, _) in enumerate(sorted_vocab_count) ] )\n",
    "        self._index2token = dict( [ (idx, w) \\\n",
    "                for w, idx in self._token2index.items() ])\n",
    "\n",
    "        for tok, idx in self._reserved_token_id:\n",
    "            self._token2index[tok] = idx\n",
    "            self._index2token[idx] = tok\n",
    "\n",
    "        if self._num_tokens > self.size:\n",
    "            self._num_tokens = self.size\n",
    "\n",
    "        self.sorted = True\n",
    "\n",
    "    def sort_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Sorts the vocabulary (if it is not already sorted).\n",
    "        \"\"\"\n",
    "        if not self.sorted:\n",
    "            self.trim()\n",
    "\n",
    "\n",
    "    def get_index(self, token):\n",
    "        \"\"\"\n",
    "        Returns: int: ID of the given token.\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return self._token2index[token]\n",
    "\n",
    "\n",
    "    def get_token(self, index):\n",
    "        \"\"\"\n",
    "        Returns: str: token with ID equal to the given index.\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return self._index2token[index]\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        Returns: int: maximum number of words in the vocabulary.\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return self._num_tokens + self._num_reserved\n",
    "\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Adds an occurrence of a token to the vocabulary,\n",
    "        incrementing its observed frequency if the word already exists.\n",
    "        Args: token (int): word to add\n",
    "        \"\"\"\n",
    "        if token in self._reserved:\n",
    "            return\n",
    "        if token not in self._token2count:\n",
    "            self._token2count[token] = 1\n",
    "            self._num_tokens += 1\n",
    "        else:\n",
    "            self._token2count[token] += 1\n",
    "        self.sorted = False\n",
    "\n",
    "    def add_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Adds a sequence of words to the vocabulary.\n",
    "        Args: sequence(list(str)): list of words, e.g. representing a sentence.\n",
    "        \"\"\"\n",
    "        for tok in sequence:\n",
    "            self.add_token(tok)\n",
    "\n",
    "    def indices_from_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Maps a list of words to their token IDs, or else <UNK>\n",
    "        if the word is rare/unknown.\n",
    "        Args: sequence (list(str)): list of words to map\n",
    "        Returns: list(int): list of mapped IDs\n",
    "        \"\"\"\n",
    "        self.sort_vocabulary()\n",
    "        return [self._token2index[tok]\n",
    "                if tok in self._token2index\n",
    "                else self.UNK_token_id\n",
    "                for tok in sequence]\n",
    "\n",
    "    def sequence_from_indices(self, indices):\n",
    "        \"\"\"\n",
    "        Recover a sentence from a list of token IDs.\n",
    "        Args: indices (list(int)): list of token IDs.\n",
    "        Returns: list(str): recovered sentence, represented as a list of words\n",
    "        \"\"\"\n",
    "        seq = [self._index2token[idx] for idx in indices]\n",
    "        return seq\n",
    "\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, self.__class__):\n",
    "            return False\n",
    "        self.sort_vocabulary()\n",
    "        other.sort_vocabulary()\n",
    "\n",
    "        if self._token2count == other._token2count \\\n",
    "                and self._token2index == other._token2index \\\n",
    "                and self._index2token == other._index2token:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self._token2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import numpy as np\n",
    "from operator import itemgetter, attrgetter\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews1 = [[1,2], [1,2,3],[6,7,8]]\n",
    "reviews2 = [[1], [1,2]]\n",
    "\n",
    "question1 = [1,2,3]\n",
    "question2 = [2,3,4,5]\n",
    "\n",
    "answer1 = [1,3]\n",
    "answer2 = [2]\n",
    "\n",
    "data1 = [(answer1), (answer2)]\n",
    "data2 = [(answer2, question1), (answer1, question2)]\n",
    "data3 = [(answer1, question1, reviews2), (answer2, question2, reviews1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3], [2]]\n",
      "[([2], [1, 2, 3]), ([1, 3], [2, 3, 4, 5])]\n",
      "[([1, 3], [1, 2, 3], [[1], [1, 2]]), ([2], [2, 3, 4, 5], [[1, 2], [1, 2, 3], [6, 7, 8]])]\n"
     ]
    }
   ],
   "source": [
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, q = zip(*data2)\n",
    "print(a)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataLoader(DataLoader):\n",
    "\n",
    "    def sortByLength(self, item):\n",
    "        if self.mode is \"1\":\n",
    "            return len(item)\n",
    "        \n",
    "        elif self.mode is \"2\":\n",
    "            assert(len(item) == 2)\n",
    "            print(item[0])\n",
    "            return len(item[0])\n",
    "        \n",
    "        elif self.mode is \"3\":\n",
    "            assert(len(item) == 3)\n",
    "            reviews = item[2]\n",
    "            max_len = 0\n",
    "            for review in reviews:\n",
    "                max_len = max(max_len, len(review))\n",
    "            return max_len\n",
    "\n",
    "        \n",
    "    def __init__(self, data, mode, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        data = sorted(data, key=self.sortByLength, reverse=True) \n",
    "        self.data = data\n",
    "        \n",
    "\n",
    "    def create_packed_qa(self, batch_data):\n",
    "        lengths = np.array([len(item) for item in batch_data])\n",
    "        max_len = max(lengths)\n",
    "        \n",
    "        padded_data = np.array( [np.pad(item, (0, max_len-len(item)), 'constant') for item in batch_data] )\n",
    "        padded_data = torch.from_numpy(padded_data)\n",
    "        \n",
    "        return (padded_data)\n",
    "    \n",
    "    def create_packed_reviews(self, review_data):\n",
    "        max_num_reviews = 0\n",
    "        for reviews in review_data:\n",
    "            max_num_reviews = max(max_num_reviews, len(reviews))\n",
    "        \n",
    "        data = []\n",
    "        for i in range(max_num_reviews):\n",
    "            batch_data = []\n",
    "            for j in range(self.batch_size):\n",
    "                reviews = review_data[j]\n",
    "                if i < len(reviews):\n",
    "                    batch_data.append(reviews[i])\n",
    "                else:\n",
    "                    batch_data.append([0])\n",
    "            data.append(batch_data)\n",
    "        \n",
    "        padded_data = []\n",
    "        for i in range(max_num_reviews):\n",
    "            batch_data = data[i]\n",
    "            lengths = [len(review) for review in batch_data]\n",
    "            max_len = max(lengths)\n",
    "            \n",
    "            padded_batch_data = np.array([np.pad(item, (0, max_len-len(item)), 'constant') for item in batch_data])\n",
    "            padded_batch_data = torch.from_numpy(padded_batch_data)\n",
    "            \n",
    "            padded_data.append(padded_batch_data)\n",
    "            \n",
    "        return padded_data\n",
    "        \n",
    "    def __iter__(self):\n",
    "        print(self.data)\n",
    "        self.num_batches = len(self.data) // self.batch_size\n",
    "        indices = np.arange(self.num_batches)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for index in indices:\n",
    "            start = index*self.batch_size\n",
    "            end = (index+1)*self.batch_size\n",
    "            \n",
    "            batch_data = self.data[start:end]\n",
    "            assert(self.batch_size == len(batch_data))\n",
    "            \n",
    "            if self.mode is \"1\":\n",
    "                answers = batch_data\n",
    "                print(answers)\n",
    "                packed_answers = self.create_packed_qa(list(answers))\n",
    "                yield (packed_answers)\n",
    "                \n",
    "            elif self.mode is \"2\":\n",
    "                answers, questions = zip(*batch_data)\n",
    "                packed_answers = self.create_packed_qa(list(answers))\n",
    "                packed_questions = self.create_packed_qa(list(questions))\n",
    "                yield (packed_answers, packed_questions)\n",
    "            \n",
    "            elif self.mode is \"3\":\n",
    "                answers, questions, reviews = zip(*batch_data)\n",
    "                packed_answers = self.create_packed_qa(list(answers))\n",
    "                packed_questions = self.create_packed_qa(list(questions))\n",
    "                packed_reviews = self.create_packed_reviews(list(reviews))\n",
    "                yield (packed_answers, packed_questions, packed_reviews)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = AmazonDataLoader(data3, \"3\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([2], [2, 3, 4, 5], [[1, 2], [1, 2, 3], [6, 7, 8]]), ([1, 3], [1, 2, 3], [[1], [1, 2]])]\n",
      "(\n",
      " 2  0\n",
      " 1  3\n",
      "[torch.LongTensor of size (2,2)]\n",
      ", \n",
      " 2  3  4  5\n",
      " 1  2  3  0\n",
      "[torch.LongTensor of size (2,4)]\n",
      ", [\n",
      " 1  2\n",
      " 1  0\n",
      "[torch.LongTensor of size (2,2)]\n",
      ", \n",
      " 1  2  3\n",
      " 1  2  0\n",
      "[torch.LongTensor of size (2,3)]\n",
      ", \n",
      " 6  7  8\n",
      " 0  0  0\n",
      "[torch.LongTensor of size (2,3)]\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader):\n",
    "    answers = data\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortByMaxReviewLength(item):\n",
    "    reviews = item[2]\n",
    "    max_len = 0\n",
    "    for review in reviews:\n",
    "        max_len = max(max_len, review.shape[0])\n",
    "    return max_len\n",
    "\n",
    "for item in data3:\n",
    "    assert(len(item) == 3)\n",
    "\n",
    "sorted(data3, key=sortByMaxReviewLength, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortByAnswerLength(item):\n",
    "    return item[0].shape[0]\n",
    "\n",
    "sorted(data2, key=sortByAnswerLength, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = data1\n",
    "print(answers)\n",
    "\n",
    "answers, questions = zip(*data2)\n",
    "print(data2)\n",
    "print(answers)\n",
    "print(questions)\n",
    "\n",
    "answers, questions, reviews = zip(*data3)\n",
    "print(data3)\n",
    "print(answers)\n",
    "print(questions)\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(10, 10, 30))\n",
    "lens = list(range(1, 11))\n",
    "x = pack_padded_sequence(x, lens[::-1], batch_first=True)\n",
    "y = pad_packed_sequence(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
