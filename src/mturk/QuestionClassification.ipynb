{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N    1754\n",
      "Y    1094\n",
      "S    1074\n",
      "Name: is_answerable_orig, dtype: int64\n",
      "Total 3922\n"
     ]
    }
   ],
   "source": [
    "filename = '../../data/final_annotated_data.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "print(df.is_answerable_orig.value_counts())\n",
    "print('Total', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Renames\n",
    "cols = ['question', 'review0', 'review1', 'review2', 'review3', 'review4', 'is_answerable', 'is_answerable_human', 'expert']\n",
    "df = df.rename(columns={'is_answerable_orig': 'is_answerable', 'is_answerable_new': 'is_answerable_human'})[cols]\n",
    "\n",
    "def filter_data(df):\n",
    "    df = df[df.is_answerable.notnull()]\n",
    "    df = df[df.review0.notnull()]\n",
    "    return df\n",
    "\n",
    "L = len(df)\n",
    "df = filter_data(df)\n",
    "assert L == len(df)\n",
    "\n",
    "\n",
    "label_map = {'N': 0, 'S': 1, 'Y': 1}\n",
    "\n",
    "def label_func(df):\n",
    "    df['label'] = df.is_answerable.apply(lambda x: label_map[x])\n",
    "\n",
    "label_func(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(df, name='df'):\n",
    "    print(name.upper())\n",
    "    print('Length = %d' % len(df))\n",
    "    print('IsAnswerable Counts')\n",
    "    print(df.is_answerable.value_counts() / len(df))\n",
    "    print('IsAnswerableHuman Counts')\n",
    "    print(df.is_answerable_human.value_counts()  / len(df))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Length = 3517\n",
      "IsAnswerable Counts\n",
      "N    0.432187\n",
      "Y    0.284049\n",
      "S    0.283765\n",
      "Name: is_answerable, dtype: float64\n",
      "IsAnswerableHuman Counts\n",
      "Series([], Name: is_answerable_human, dtype: float64)\n",
      "\n",
      "VAL\n",
      "Length = 200\n",
      "IsAnswerable Counts\n",
      "N    0.625\n",
      "S    0.200\n",
      "Y    0.175\n",
      "Name: is_answerable, dtype: float64\n",
      "IsAnswerableHuman Counts\n",
      "Series([], Name: is_answerable_human, dtype: float64)\n",
      "\n",
      "TEST\n",
      "Length = 205\n",
      "IsAnswerable Counts\n",
      "N    0.531707\n",
      "Y    0.292683\n",
      "S    0.175610\n",
      "Name: is_answerable, dtype: float64\n",
      "IsAnswerableHuman Counts\n",
      "N    0.278049\n",
      "Y    0.258537\n",
      "S    0.121951\n",
      "Name: is_answerable_human, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = df[df.expert]\n",
    "train_df_part1 = df[~df.expert]\n",
    "\n",
    "test_df_part1 = test_df[test_df.is_answerable_human.notnull()]\n",
    "expert_rest_df = test_df[test_df.is_answerable_human.isnull()]\n",
    "\n",
    "validate_df, test_df_part2, train_df_part2 = np.split(expert_rest_df.sample(frac=1), [200, 270])\n",
    "\n",
    "train_df = pd.concat([train_df_part1, train_df_part2]).sample(frac=1)\n",
    "test_df = pd.concat([test_df_part1, test_df_part2]).sample(frac=1)\n",
    "\n",
    "print_stats(train_df, 'train')\n",
    "print_stats(validate_df, 'val')\n",
    "print_stats(test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_reviews(row):\n",
    "    all_reviews = ''\n",
    "    for key in ['review0', 'review1', 'review2', 'review3', 'review4']:\n",
    "        if not isinstance(row[key], float):\n",
    "            all_reviews += row[key].strip(' ').strip('-')\n",
    "            all_reviews += ' '\n",
    "    return all_reviews.strip()\n",
    "\n",
    "\n",
    "def add_reviews(df):\n",
    "    df['reviews'] = df.apply(lambda x: get_reviews(x), axis = 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df, test_df, validate_df = list(map(add_reviews, [train_df, test_df, validate_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>review0</th>\n",
       "      <th>review1</th>\n",
       "      <th>review2</th>\n",
       "      <th>review3</th>\n",
       "      <th>review4</th>\n",
       "      <th>is_answerable</th>\n",
       "      <th>is_answerable_human</th>\n",
       "      <th>expert</th>\n",
       "      <th>label</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>do you need an amp</td>\n",
       "      <td>The speakers are attractive; they look at leas...</td>\n",
       "      <td>TRASH! They look nice, but I've received two p...</td>\n",
       "      <td>These speakers are not \"Rockers\", you would ne...</td>\n",
       "      <td>Very nice speakers that look cool and have a g...</td>\n",
       "      <td>This is the main reason I will be returning th...</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>The speakers are attractive; they look at leas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>can you hook two together?</td>\n",
       "      <td>while i know this is meant for floor vents, i ...</td>\n",
       "      <td>i am using this in a different way.i bought a ...</td>\n",
       "      <td>We have a large chest of drawers sitting over ...</td>\n",
       "      <td>I didn't need to get this as it turned out but...</td>\n",
       "      <td>-</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>while i know this is meant for floor vents, i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>What kind of receiver are you using with these...</td>\n",
       "      <td>I owned a pair of these speakers which worked ...</td>\n",
       "      <td>I got these as a birthday present and once ins...</td>\n",
       "      <td>...if you have thousands of dollars invested i...</td>\n",
       "      <td>The 151's were already at a moderate listening...</td>\n",
       "      <td>You could 'feel' the sound everywhere.However,...</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>I owned a pair of these speakers which worked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>can it works in the sony vaio t series</td>\n",
       "      <td>Corsair has a good name for computer accessori...</td>\n",
       "      <td>My laptop has an i3-2310M @ 2.10GHz with (2x2G...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Corsair has a good name for computer accessori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>I want to make a cradle for a water barrel and...</td>\n",
       "      <td>I now have 110 gallons of long term stored eme...</td>\n",
       "      <td>I purchased this barrel for long term water st...</td>\n",
       "      <td>I live in a townhouse in Earthquake and Hurric...</td>\n",
       "      <td>I've recently been through two disasters and l...</td>\n",
       "      <td>-</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>I now have 110 gallons of long term stored eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "970                                  do you need an amp   \n",
       "3222                         can you hook two together?   \n",
       "2487  What kind of receiver are you using with these...   \n",
       "1552             can it works in the sony vaio t series   \n",
       "358   I want to make a cradle for a water barrel and...   \n",
       "\n",
       "                                                review0  \\\n",
       "970   The speakers are attractive; they look at leas...   \n",
       "3222  while i know this is meant for floor vents, i ...   \n",
       "2487  I owned a pair of these speakers which worked ...   \n",
       "1552  Corsair has a good name for computer accessori...   \n",
       "358   I now have 110 gallons of long term stored eme...   \n",
       "\n",
       "                                                review1  \\\n",
       "970   TRASH! They look nice, but I've received two p...   \n",
       "3222  i am using this in a different way.i bought a ...   \n",
       "2487  I got these as a birthday present and once ins...   \n",
       "1552  My laptop has an i3-2310M @ 2.10GHz with (2x2G...   \n",
       "358   I purchased this barrel for long term water st...   \n",
       "\n",
       "                                                review2  \\\n",
       "970   These speakers are not \"Rockers\", you would ne...   \n",
       "3222  We have a large chest of drawers sitting over ...   \n",
       "2487  ...if you have thousands of dollars invested i...   \n",
       "1552                                                  -   \n",
       "358   I live in a townhouse in Earthquake and Hurric...   \n",
       "\n",
       "                                                review3  \\\n",
       "970   Very nice speakers that look cool and have a g...   \n",
       "3222  I didn't need to get this as it turned out but...   \n",
       "2487  The 151's were already at a moderate listening...   \n",
       "1552                                                  -   \n",
       "358   I've recently been through two disasters and l...   \n",
       "\n",
       "                                                review4 is_answerable  \\\n",
       "970   This is the main reason I will be returning th...             S   \n",
       "3222                                                  -             N   \n",
       "2487  You could 'feel' the sound everywhere.However,...             N   \n",
       "1552                                                  -             N   \n",
       "358                                                   -             N   \n",
       "\n",
       "     is_answerable_human  expert  label  \\\n",
       "970                  NaN   False      1   \n",
       "3222                 NaN   False      0   \n",
       "2487                 NaN   False      0   \n",
       "1552                 NaN   False      0   \n",
       "358                  NaN   False      0   \n",
       "\n",
       "                                                reviews  \n",
       "970   The speakers are attractive; they look at leas...  \n",
       "3222  while i know this is meant for floor vents, i ...  \n",
       "2487  I owned a pair of these speakers which worked ...  \n",
       "1552  Corsair has a good name for computer accessori...  \n",
       "358   I now have 110 gallons of long term stored eme...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_csv('../../data/train-qar_sample_100000.csv')\\ndf = add_reviews(df)\\nprint(len(df))\\n\\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\\ntfidf_vectorizer.fit(list(df.question.values) + list(df.reviews.values))\\n\\nwith open('../../data/tfidf_vectorizer.pkl', 'wb') as fp:\\n    pickle.dump(tfidf_vectorizer, fp)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = pd.read_csv('../../data/train-qar_sample_100000.csv')\n",
    "df = add_reviews(df)\n",
    "print(len(df))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorizer.fit(list(df.question.values) + list(df.reviews.values))\n",
    "\n",
    "with open('../../data/tfidf_vectorizer.pkl', 'wb') as fp:\n",
    "    pickle.dump(tfidf_vectorizer, fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/tfidf_vectorizer.pkl', 'rb') as fp:\n",
    "    tfidf_vectorizer = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158752"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w2v Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/w2v_vectorizer.pkl', 'rb') as fp:\n",
    "    w2v_vectorizer = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\n\\nwith open(\"../../data/glove.6B.300d.txt\", \"rb\") as lines:\\n    w2v = {str(line.split()[0].decode(\\'UTF-8\\')): np.array(list(map(float, line.split()[1:])))\\n           for line in lines}\\n\\nw2v_vectorizer = MeanEmbeddingVectorizer(w2v)\\nwith open(\\'../../data/w2v_vectorizer.pkl\\', \\'wb\\') as fp:\\n    pickle.dump(w2v_vectorizer, fp)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "with open(\"../../data/glove.6B.300d.txt\", \"rb\") as lines:\n",
    "    w2v = {str(line.split()[0].decode('UTF-8')): np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}\n",
    "\n",
    "w2v_vectorizer = MeanEmbeddingVectorizer(w2v)\n",
    "with open('../../data/w2v_vectorizer.pkl', 'wb') as fp:\n",
    "    pickle.dump(w2v_vectorizer, fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_raw, test_df_raw, validate_df_raw = list(map(lambda x: x.copy(), [train_df, test_df, validate_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    punctuations = string.punctuation.replace(\"\\'\", '')\n",
    "\n",
    "    for ch in punctuations:\n",
    "        text = text.replace(ch, \" \" + ch + \" \")\n",
    "\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if not token.isupper():\n",
    "            tokens[i] = token.lower()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = vectorizer.transform([\\'It is a total idiot\\'])\\ny = vectorizer.transform([\\'It is not a total idiocy\\'])\\n\\nprint(type(x), \\'|\\n\\', x.shape, \\'|\\n\\', x, \\'|\\n\\', len(x.toarray()[0]))\\nprint(\"-\"*50)\\nprint(type(y), \\'|\\n\\', y.shape, \\'|\\n\\', y, \\'|\\n\\', len(y.toarray()[0]))\\nprint(\"-\"*50)\\nprint(x.toarray().dot(y.toarray().transpose())[0][0])\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x = vectorizer.transform(['It is a total idiot'])\n",
    "y = vectorizer.transform(['It is not a total idiocy'])\n",
    "\n",
    "print(type(x), '|\\n', x.shape, '|\\n', x, '|\\n', len(x.toarray()[0]))\n",
    "print(\"-\"*50)\n",
    "print(type(y), '|\\n', y.shape, '|\\n', y, '|\\n', len(y.toarray()[0]))\n",
    "print(\"-\"*50)\n",
    "print(x.toarray().dot(y.toarray().transpose())[0][0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_intersection(q, r):\n",
    "    return len(set(q).intersection(set(r)))\n",
    "\n",
    "def w2v_sim(q, r):\n",
    "    # dot product of q and r as w2v vectors\n",
    "    q_vec = w2v_vectorizer.transform([q])\n",
    "    r_vec = w2v_vectorizer.transform([r])\n",
    "    return q_vec.dot(r_vec.transpose())[0][0]\n",
    "\n",
    "def tf_idf_sim(q, r):\n",
    "    # dot product of q and r as tfidf vectors\n",
    "    q_vec = tfidf_vectorizer.transform([q])\n",
    "    r_vec = tfidf_vectorizer.transform([r])\n",
    "    return q_vec.dot(r_vec.transpose()).toarray()[0][0]\n",
    "\n",
    "def tf_idf_sim_sentence(q, rs):\n",
    "    # max of dot products of q and each sentence in r as tfidf vectors\n",
    "    q_vec = tfidf_vectorizer.transform([q])\n",
    "    if len(rs) == 0:\n",
    "        return 0\n",
    "    return max([q_vec.dot(tfidf_vectorizer.transform([r]).transpose()).toarray()[0][0] for r in rs])\n",
    "\n",
    "def w2v_sim_sentence(q, rs):\n",
    "    # max of dot products of q and each sentence in r as tfidf vectors\n",
    "    q_vec = w2v_vectorizer.transform([q])\n",
    "    if len(rs) == 0:\n",
    "        return 0\n",
    "    return max([q_vec.dot(w2v_vectorizer.transform([r]).transpose())[0][0] for r in rs])\n",
    "\n",
    "def tf_idf_sim_sentence_mean(q, rs):\n",
    "    # max of dot products of q and each sentence in r as tfidf vectors\n",
    "    q_vec = tfidf_vectorizer.transform([q])\n",
    "    if len(rs) == 0:\n",
    "        return 0\n",
    "    return np.mean([q_vec.dot(tfidf_vectorizer.transform([r]).transpose()).toarray()[0][0] for r in rs])\n",
    "\n",
    "def w2v_sim_sentence_mean(q, rs):\n",
    "    # max of dot products of q and each sentence in r as tfidf vectors\n",
    "    q_vec = w2v_vectorizer.transform([q])\n",
    "    if len(rs) == 0:\n",
    "        return 0\n",
    "    return np.mean([q_vec.dot(w2v_vectorizer.transform([r]).transpose())[0][0] for r in rs])\n",
    "\n",
    "def add_features(df):\n",
    "    df['q_tokens'] = df.question.apply(lambda x: tokenize(x))\n",
    "    df['r_tokens'] = df.reviews.apply(lambda x: tokenize(x))\n",
    "    df['r_sents'] = df.reviews.apply(lambda x: sent_tokenize(x))\n",
    "    df['n_q'] = df.q_tokens.apply(lambda x: len(x))\n",
    "    df['n_r'] = df.r_tokens.apply(lambda x: len(x))\n",
    "    df['n_intersection'] = df.apply(lambda x: len(set(x.q_tokens).intersection(set(x.r_tokens))), axis=1)\n",
    "    df['intr_frac'] = df.n_intersection / df.n_q\n",
    "    df['tfidf'] = df.apply(lambda x: tf_idf_sim(x.question, x.reviews), axis=1)\n",
    "    df['w2v'] = df.apply(lambda x: w2v_sim(x.question, x.reviews), axis=1)\n",
    "    df['w2v_sent'] = df.apply(lambda x: w2v_sim_sentence(x.question, x.r_sents), axis=1)\n",
    "    df['tfidf_sent'] = df.apply(lambda x: tf_idf_sim_sentence(x.question, x.r_sents), axis=1)\n",
    "    df['w2v_sent_mean'] = df.apply(lambda x: w2v_sim_sentence_mean(x.question, x.r_sents), axis=1)\n",
    "    df['tfidf_sent_mean'] = df.apply(lambda x: tf_idf_sim_sentence_mean(x.question, x.r_sents), axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, validate_df = list(map(add_features, [train_df_raw, test_df_raw, validate_df_raw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for typ, df in [('train', train_df), ('test', test_df), ('val', validate_df)]:\n",
    "    pickle.dump(df, open('../../data/' + typ + '_df_features.pkl', 'wb'))\n",
    "\n",
    "train_df_raw, test_df_raw, validate_df_raw = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['n_q', 'n_r', 'n_intersection', 'intr_frac']\n",
    "Y_cols = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df\n",
    "\n",
    "q = df.question.iloc[1]\n",
    "r = df.reviews.iloc[1]\n",
    "q_tokens = df.q_tokens.iloc[1]\n",
    "r_tokens = df.r_tokens.iloc[1]\n",
    "\n",
    "# q, r, set(q_tokens).intersection(set(r_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_q</th>\n",
       "      <th>n_r</th>\n",
       "      <th>n_intersection</th>\n",
       "      <th>intr_frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.272368</td>\n",
       "      <td>332.815789</td>\n",
       "      <td>7.380263</td>\n",
       "      <td>0.472840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.484226</td>\n",
       "      <td>380.175263</td>\n",
       "      <td>9.612419</td>\n",
       "      <td>0.584565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             n_q         n_r  n_intersection  intr_frac\n",
       "label                                                  \n",
       "0      16.272368  332.815789        7.380263   0.472840\n",
       "1      17.484226  380.175263        9.612419   0.584565"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[Y_cols] + X_cols].groupby(Y_cols).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-744f628ec572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_cols_all\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[1;32m    912\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[0;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[1;32m    764\u001b[0m     raise ValueError('Unsupported set of arguments: %s, '\n\u001b[1;32m    765\u001b[0m                      \u001b[0;34m'Parameters: penalty=%r, loss=%r, dual=%r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m                      % (error_string, penalty, loss, dual))\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "extra_features = ['w2v', 'tfidf']\n",
    "X_cols_all = X_cols + ['w2v_sent', 'w2v_sent_mean', 'tfidf_sent', 'tfidf_sent_mean'] + extra_features\n",
    "\n",
    "models = []\n",
    "models.append(LinearSVC(penalty='l1'))\n",
    "models.append(LogisticRegression(C=100))\n",
    "models.append(DecisionTreeClassifier(max_depth=4))\n",
    "models.append(RandomForestClassifier(n_estimators=3, max_depth=4))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    models[i].fit(train_df[X_cols_all].values, train_df[Y_cols].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5dce22337efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mplot_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'modelss' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_curves(model):\n",
    "    probs = model.predict_proba(validate_df[X_colss[2]].values)[:,1]\n",
    "    ytrue = validate_df[Y_cols].values\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(ytrue, probs, pos_label=1)\n",
    "    score = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(ytrue, probs)\n",
    "    print(precision, recall, thresholds)\n",
    "    plt.figure()\n",
    "    plt.plot(precision, recall, color='darkorange')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    thresholds = [0.25, 0.5, 0.75]\n",
    "    for th in thresholds:\n",
    "        #print(probs)\n",
    "        predictions = [1 if prob > th else 0 for prob in probs]\n",
    "        #predictions = model.predict(validate_df[X_cols].values)\n",
    "        print(pd.Series(predictions).value_counts())\n",
    "        print(classification_report(ytrue, predictions))\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "plot_curves(modelss[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This LinearSVC instance is not fitted yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-a3028faf10b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_cols_all\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coef_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             raise NotFittedError(\"This %(name)s instance is not fitted \"\n\u001b[0;32m--> 255\u001b[0;31m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LinearSVC instance is not fitted yet"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(\"TRAIN\")\n",
    "    predictions = model.predict(train_df[X_cols_all].values)\n",
    "    print(pd.Series(predictions).value_counts())\n",
    "    print(classification_report(train_df[Y_cols].values, predictions))  \n",
    "    print(\"VAL\")\n",
    "    predictions = model.predict(validate_df[X_cols_all].values)\n",
    "    print(pd.Series(predictions).value_counts())\n",
    "    print(classification_report(validate_df[Y_cols].values, predictions))    \n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "True     71\n",
      "False    64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.69      0.70        65\n",
      "          1       0.72      0.73      0.72        70\n",
      "\n",
      "avg / total       0.71      0.71      0.71       135\n",
      "\n",
      "0 1\n",
      "True     70\n",
      "False    65\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.71      0.71        65\n",
      "          1       0.73      0.73      0.73        70\n",
      "\n",
      "avg / total       0.72      0.72      0.72       135\n",
      "\n",
      "0 2\n",
      "False    74\n",
      "True     61\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.77      0.72        65\n",
      "          1       0.75      0.66      0.70        70\n",
      "\n",
      "avg / total       0.72      0.71      0.71       135\n",
      "\n",
      "0 3\n",
      "True     73\n",
      "False    62\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.65      0.66        65\n",
      "          1       0.68      0.71      0.70        70\n",
      "\n",
      "avg / total       0.68      0.68      0.68       135\n",
      "\n",
      "1 0\n",
      "True     73\n",
      "False    62\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.65      0.66        65\n",
      "          1       0.68      0.71      0.70        70\n",
      "\n",
      "avg / total       0.68      0.68      0.68       135\n",
      "\n",
      "1 1\n",
      "True     73\n",
      "False    62\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.65      0.66        65\n",
      "          1       0.68      0.71      0.70        70\n",
      "\n",
      "avg / total       0.68      0.68      0.68       135\n",
      "\n",
      "1 2\n",
      "False    70\n",
      "True     65\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.74      0.71        65\n",
      "          1       0.74      0.69      0.71        70\n",
      "\n",
      "avg / total       0.71      0.71      0.71       135\n",
      "\n",
      "1 3\n",
      "False    70\n",
      "True     65\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.72      0.70        65\n",
      "          1       0.72      0.67      0.70        70\n",
      "\n",
      "avg / total       0.70      0.70      0.70       135\n",
      "\n",
      "2 0\n",
      "False    71\n",
      "True     64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.72      0.69        65\n",
      "          1       0.72      0.66      0.69        70\n",
      "\n",
      "avg / total       0.69      0.69      0.69       135\n",
      "\n",
      "2 1\n",
      "False    71\n",
      "True     64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.72      0.69        65\n",
      "          1       0.72      0.66      0.69        70\n",
      "\n",
      "avg / total       0.69      0.69      0.69       135\n",
      "\n",
      "2 2\n",
      "True     81\n",
      "False    54\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.55      0.61        65\n",
      "          1       0.64      0.74      0.69        70\n",
      "\n",
      "avg / total       0.65      0.65      0.65       135\n",
      "\n",
      "2 3\n",
      "True     72\n",
      "False    63\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.66      0.67        65\n",
      "          1       0.69      0.71      0.70        70\n",
      "\n",
      "avg / total       0.69      0.69      0.69       135\n",
      "\n",
      "3 0\n",
      "True     72\n",
      "False    63\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.68      0.69        65\n",
      "          1       0.71      0.73      0.72        70\n",
      "\n",
      "avg / total       0.70      0.70      0.70       135\n",
      "\n",
      "3 1\n",
      "True     71\n",
      "False    64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.69      0.70        65\n",
      "          1       0.72      0.73      0.72        70\n",
      "\n",
      "avg / total       0.71      0.71      0.71       135\n",
      "\n",
      "3 2\n",
      "True     77\n",
      "False    58\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.62      0.65        65\n",
      "          1       0.68      0.74      0.71        70\n",
      "\n",
      "avg / total       0.68      0.68      0.68       135\n",
      "\n",
      "3 3\n",
      "True     71\n",
      "False    64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.66      0.67        65\n",
      "          1       0.69      0.70      0.70        70\n",
      "\n",
      "avg / total       0.68      0.68      0.68       135\n",
      "\n",
      "4 0\n",
      "True     69\n",
      "False    66\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.72      0.72        65\n",
      "          1       0.74      0.73      0.73        70\n",
      "\n",
      "avg / total       0.73      0.73      0.73       135\n",
      "\n",
      "4 1\n",
      "True     68\n",
      "False    67\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.68      0.67        65\n",
      "          1       0.69      0.67      0.68        70\n",
      "\n",
      "avg / total       0.67      0.67      0.67       135\n",
      "\n",
      "4 2\n",
      "True     86\n",
      "False    49\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.54      0.61        65\n",
      "          1       0.65      0.80      0.72        70\n",
      "\n",
      "avg / total       0.68      0.67      0.67       135\n",
      "\n",
      "4 3\n",
      "True     87\n",
      "False    48\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.55      0.64        65\n",
      "          1       0.67      0.83      0.74        70\n",
      "\n",
      "avg / total       0.71      0.70      0.69       135\n",
      "\n",
      "5 0\n",
      "True     72\n",
      "False    63\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.68      0.69        65\n",
      "          1       0.71      0.73      0.72        70\n",
      "\n",
      "avg / total       0.70      0.70      0.70       135\n",
      "\n",
      "5 1\n",
      "True     71\n",
      "False    64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.69      0.70        65\n",
      "          1       0.72      0.73      0.72        70\n",
      "\n",
      "avg / total       0.71      0.71      0.71       135\n",
      "\n",
      "5 2\n",
      "True     73\n",
      "False    62\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.63      0.65        65\n",
      "          1       0.67      0.70      0.69        70\n",
      "\n",
      "avg / total       0.67      0.67      0.67       135\n",
      "\n",
      "5 3\n",
      "True     75\n",
      "False    60\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.66      0.69        65\n",
      "          1       0.71      0.76      0.73        70\n",
      "\n",
      "avg / total       0.71      0.71      0.71       135\n",
      "\n",
      "6 0\n",
      "True     71\n",
      "False    64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.69      0.70        65\n",
      "          1       0.72      0.73      0.72        70\n",
      "\n",
      "avg / total       0.71      0.71      0.71       135\n",
      "\n",
      "6 1\n",
      "True     71\n",
      "False    64\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.69      0.70        65\n",
      "          1       0.72      0.73      0.72        70\n",
      "\n",
      "avg / total       0.71      0.71      0.71       135\n",
      "\n",
      "6 2\n",
      "False    74\n",
      "True     61\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.77      0.72        65\n",
      "          1       0.75      0.66      0.70        70\n",
      "\n",
      "avg / total       0.72      0.71      0.71       135\n",
      "\n",
      "6 3\n",
      "True     79\n",
      "False    56\n",
      "dtype: int64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.60      0.64        65\n",
      "          1       0.67      0.76      0.71        70\n",
      "\n",
      "avg / total       0.68      0.68      0.68       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def predictions_k(model, X, k):\n",
    "    probs = model.predict_proba(X)\n",
    "    return probs[:, 1] >= k\n",
    "\n",
    "for j, models in enumerate(modelss):\n",
    "    for i, model in enumerate(models):\n",
    "        print(j ,i)\n",
    "        predictions = model.predict(test_df1[X_colss[j]].values)\n",
    "        predictions = predictions_k(model, test_df1[X_colss[j]].values, 0.6)\n",
    "        print(pd.Series(predictions).value_counts())\n",
    "        print(classification_report(test_df1[Y_cols].values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_df_human.is_answerable_human.apply(lambda x: label_map[x]).values\n",
    "labels = test_df_human.is_answerable.apply(lambda x: label_map[x]).values\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../../data/model_answerability.pkl', 'wb') as fp:\n",
    "#    pickle.dump(modelss[4][2], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/model_answerability.pkl', 'rb') as fp:\n",
    "    model = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
